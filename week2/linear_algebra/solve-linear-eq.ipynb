{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A linear algebra perspective on linear regression.\n",
    "\n",
    "In this notebook, we focus on solving systems of equations by using linear algebra. In particular, we represent these systems of equations as matrices, and analyze properties of these matrices in order to understand when they have solutions. In the last part of this notebook, we connect these ideas to linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a system\n",
    "\n",
    "Let's start with this simple system of equations and try to solve it by hand.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 3 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 2 \\\\\n",
    "x_2 + x_3 &= 2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*Exercise*: Try solving this system of equations. The answer is below.\n",
    "\n",
    "1. We can solve for $x_1$ by subtracting the line 3 from line 1, so $x_1=1$.\n",
    "2. We can plug that into line 2, so our system of equations is now\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 &= 1 \\\\\n",
    "- x_2 + 2 x_3 &= 1 \\\\\n",
    "x_2 + x_3 &= 2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "3. We add line 3 to line 2. That leaves us with $3 x_3 = 3$, so $x_3 = 1$.\n",
    "4. That means $x_2=1$! So our solution is $\\mathbf{x} = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$.\n",
    "\n",
    "We just followed an ad-hoc algorithm to solve this matrix, but there are more systematic ways to solve it by 1) systematically repeating simplification steps (called Gaussian eliminiation) until 2) each row reaches a _canonical_ form (reduced row-echelon form). We won't cover this in any detail, but just point out that there are systematic algorithms that can do it, with preferable properties (efficiency, numerical stability).\n",
    "\n",
    "*Fun fact:*\n",
    "> *The method of Gaussian elimination appears – albeit without proof – in the Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 AD, but parts of it were written as early as approximately 150 BC.It was commented on by Liu Hui in the 3rd century.* [Wikipedia](https://en.wikipedia.org/wiki/Gaussian_elimination#History)\n",
    "\n",
    "What do these systems of equations have in common with linear regression? They have some clear similarities (solving for linear weights). But also some clear differences (regression is noisy, we rarely have the same number of observations and features). The goal of these notes is to understand the linear algebra behind regression, in order to understand more about regression and when it does or doesn't have solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more problematic system\n",
    "\n",
    "Now, let's examine a different linear system to see some of the different kinds of solutions they can have.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 3 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 2 \\\\\n",
    "2 x_1 + 3 x_3 &= 1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If you add line 2 to line 1, you get $2 x_1 + 3 x_3 = 5$, which contradicts line 3! So, there's no way to solve this system of equations, making them inconsistent. But what happens if we change this to avoid that contradiction? If we only modify the last equation to avoid the contradiction, we get this system (change in bold)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 3 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 2 \\\\\n",
    "2 x_1 + 3 x_3 &= \\textbf{5} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "From this, we can reduce to two equations, obtaining the first by subtracting line 2 from line 1\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "2 x_2 - x_3 &= 1 \\\\\n",
    "2 x_1 + 3 x_3 &= 5 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Any $\\textbf{x}$ that satisfies these equations is a solution, like $\\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$ or $\\begin{bmatrix} -2 & 2 & 3 \\end{bmatrix}$. In fact, there are infinitely many solutions!\n",
    "\n",
    "In our example in the previous section, the system of equations seemed to have a single unique solution. In this example, depending on the $\\textbf{b}$ on the right, its possible to have either zero or infinite solutions. In the coming sections, we'll try to analyze the differences of these matrices to explain these patterns of solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear systems as matrices\n",
    "\n",
    "In this section, we write out some notation to relate the above systems of equations with the matrix equation $\\textbf{A}\\mathbf{x}=\\mathbf{b}$.\n",
    "\n",
    "First, some notation about matrices. Consider a matrix $\\textbf{A}$ of size $n \\times m$, which means it has $n$ rows and $m$ columns.\n",
    "We can write it out in terms of the individual entries of the matrix. The entry at row $i$ and column $j$ is $a_{ij}$, so the matrix is\n",
    "\n",
    "$$\n",
    "\\textbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\ldots & a_{1m} \\\\\n",
    "a_{21} & a_{22} & \\ldots & a_{2m} \\\\\n",
    "\\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "a_{n1} & a_{n2} & \\ldots & a_{nm} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Another way we can write it is in terms of the columns. The $i^{th}$ column is $\\textbf{a}_i$, so the matrix is\n",
    "\n",
    "$$\n",
    "\\textbf{A} = \\begin{pmatrix} \\textbf{a}_1 & \\textbf{a}_2 & \\ldots & \\textbf{a}_m \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So, if we go back to our above example equations, we can rewrite them using some of these notational ideas. Here's the system of equations from the first section:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 3 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 2 \\\\\n",
    "x_2 + x_3 &= 2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Each of these equations can correspond to a dot-product between $\\textbf{x}$ and a vector with the factors (e.g., $\\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$ for the first row). So, we can write this as a matrix equation\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & -1 & 2 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "\\end{bmatrix} \\mathbf{x} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Another different approach is to write it as a vector equation, using the columns of the above matrix.\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} x_1 +\n",
    "\\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} x_2 +\n",
    "\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} x_3\n",
    "= \\begin{bmatrix} 3 \\\\ 2 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "All three of these representations are equivalent, and are all solved by $\\textbf{x} = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$. In future sections, we'll freely switch between these representations as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the contradiction\n",
    "\n",
    "Now, we're going to further analyze the above case where we reached a contradiction.\n",
    "When we were solving system of equations above, we were trying to find a solution $\\textbf{x}$ that satisfies the\n",
    "the equation $\\textbf{A}\\textbf{x}=\\textbf{b}$, given a specific matrix $\\textbf{A}$ and vector $\\textbf{b}$. From our above examples, a natural followup is to generalize to arbitrary $\\textbf{b}$ by asking: Given a matrix $\\textbf{A}$, can we say something general about the $\\textbf{b}$ that will have a solution?\n",
    "\n",
    "Let's look at our problematic example above, now in matrix form:\n",
    "$$\n",
    "\\textbf{A} = \\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & -1 & 2 \\\\\n",
    "2 & 0 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can rewrite the equation $\\textbf{A}\\textbf{x}=\\textbf{b}$ in a vectorized form as follows\n",
    "\n",
    "$$\n",
    "\\textbf{a}_1 x_1 + \\textbf{a}_2 x_2 + \\ldots + \\textbf{a}_m x_m = \\textbf{b}\n",
    "$$\n",
    "\n",
    "Or, more specifically for our current example\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix} x_1 +\n",
    "\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix} x_2 +\n",
    "\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} x_3\n",
    "= \\textbf{b}\n",
    "$$\n",
    "\n",
    "This makes it clear that $\\textbf{b}$ is a **linear combination** of the columns of $\\textbf{A}$, weighted by $\\textbf{x}$. A linear combination is a weighted sum of vectors. In this case, the weights are the elements of $\\textbf{x}$, and the vectors that we combine are the columns of $\\textbf{A}$. A related concept is the **span** of a generating set of vectors, which is the set of all possible linear combinations of the vectors. One final concept: The **column space** of a matrix, $col(\\textbf{A})$ is the span of its columns. So, we can rephrase the above as: vectors $\\textbf{b}$ with a solution must be in $col(\\textbf{A})$, the column space of $\\textbf{A}$.\n",
    "\n",
    "So that explains part of what we saw above: we reached a contradiction with $\\textbf{b} = \\begin{bmatrix} 3 & 2 & 1 \\end{bmatrix}$ because it cannot be expressed as a linear combination of the columns of $\\textbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite solutions?\n",
    "\n",
    "We have another property of our problematic matrix left to explain: the infinite number of solutions for certain $\\textbf{x}$.\n",
    "To explain this, we introduce the **null space**, or $null(\\textbf{A})$, which are the solutions $\\textbf{x'}$ so that $\\textbf{A}\\textbf{x'} = \\textbf{0}$. Every matrix has the trivial solution $\\textbf{x'}=\\textbf{0}$, but only some matrices have other non-trivial solutions. Given a known solution $\\textbf{x}$ and non-trivial element of the null space $\\textbf{x'}$, you can construct another solution to by adding these equations together, since\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{A}\\textbf{x} + \\textbf{A}\\textbf{x'} &= \\textbf{b} + \\textbf{0} \\\\\n",
    "\\textbf{A}(\\textbf{x} + \\textbf{x'}) &= \\textbf{b} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So that means $\\textbf{x}+\\textbf{x'}$ is another solution. Since knowing the null space is a way to generate more solutions, let's see if we can solve $\\textbf{A}\\textbf{x} =\\textbf{0}$ for our above matrices. We'll work with our problematic example in equation form.\n",
    "\n",
    "*Exercise*: Solve $\\textbf{A}\\textbf{x} =\\textbf{0}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 0 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 0 \\\\\n",
    "2 x_1 + 3 x_3 &= 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we subtract line 2 from line 1 like we did above, we get the following constraints\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "2 x_2 - x_3 &= 0 \\\\\n",
    "2 x_1 + 3 x_3 &= 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "These constraints are satisfied by solutions like $\\begin{bmatrix} -3 & 1 & 2 \\end{bmatrix}$ or $\\begin{bmatrix} -6 & 2 & 4 \\end{bmatrix}$. So, it looks like the null space is the span of a set containing the vector $\\begin{bmatrix} -3 & 1 & 2 \\end{bmatrix}$.\n",
    "\n",
    "And, if we look at the above case of infinite solutions ($\\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$, $\\begin{bmatrix} -2 & 2 & 3 \\end{bmatrix}$), you'll notice that their difference is actually $\\begin{bmatrix} -3 & 1 & 2 \\end{bmatrix}$. This example shows that, given a solution $\\textbf{x}$ to $\\textbf{A}\\textbf{x}=\\textbf{b}$ and a non-trivial null space, you can produce an infinite number of solutions $\\textbf{x} + \\textbf{x}'$ for all $x' \\in null(\\textbf{A})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A fundamental theorem\n",
    "\n",
    "For our problematic matrix, we now have an explanation for cases where we reach the contradiction (the element is not in the span column space) and an explanation for cases where their infinite solutions (the null space is non-trivial, and can generate infinite solutions). These two properties of our problematic metrics are fundamentally related, which we'll explain now. But first, we'll introduce some helpful concepts.\n",
    "\n",
    "A set of vectors is **linearly dependent** if one vector can be written as a linear combination of the others.\n",
    "For example, let's look again at our problematic matrix:\n",
    "$$\n",
    "\\textbf{A} = \\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & -1 & 2 \\\\\n",
    "2 & 0 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can write one column as a linear combination of the other two:\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix} =\n",
    "3 \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix} -\n",
    "2 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*Exercise: How are the coefficients of this linear combination related to elements of the null space above? Why?*\n",
    "\n",
    "So, our three columns are redundant! By removing the vector $\\begin{bmatrix} 1 & -1 & 0 \\end{bmatrix}$, we can remove this redundancy. After removing vectors that can be written as a linear combination of others, the remaining ones are **linearly independent**.\n",
    "\n",
    "The redundancy in columns means that, even though we are working with three-dimensional vectors, our vectors are in a two-dimensional space defined by the linearly independent vectors. To keep these concepts separate, we introduce a new concept: the **dimensionality** of a space, $dim(\\cdot)$. So our problemetic matrix $\\textbf{A}$, which has a linearly independent set of two column vectors, has a two-dimensional column space, which we can write out as $dim(col(\\textbf{A}))=2$.\n",
    "\n",
    "We can now write the fundamental theorem of linear algebra\n",
    "\n",
    "$$\n",
    "size(\\textbf{A}) = dim(col(\\textbf{A})) + dim(null(\\textbf{A}))\n",
    "$$\n",
    "\n",
    "where we have assumed a square matrix $\\textbf{A}$ and the size of the matrix is $size(\\cdot)$. The idea here is that there are only two possible cases for the result of $\\textbf{A}\\textbf{x}$, when $\\textbf{x} \\ne \\textbf{0}$. It either results in $\\textbf{0}$, so $\\textbf{x} \\in null(\\textbf{A})$. Or, it results in a non-trivial $\\textbf{b} \\in col(\\textbf{A})$.\n",
    "\n",
    "One final definition: We give a special name to the dimensionality of the column space of a matrix $\\textbf{A}$: the **rank** of the matrix. So, $rank(\\textbf{A}) = dim(col(\\textbf{A}))$. It's an important concept because it tells us something intrinsic about the matrix: how much information about $\\textbf{x}$ is retained (i.e. mapped to the column space) by the result of $\\textbf{A}\\textbf{x}$. When a matrix has a rank that matches the dimensionality, so that $rank(\\textbf{A})=size(\\textbf{A})$, we call it **full-rank**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The unproblematic matrix\n",
    "\n",
    "So, going back to our very first example, what was special about that system of equations? Let's try to compute its null space.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 0 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 0 \\\\\n",
    "x_2 + x_3 &= 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We follow the same sequence of simplification steps as above, so\n",
    "\n",
    "1. Subtract line 3 from line 1, so $x_1=0$.\n",
    "2. Plug $x_1=0$ into line 2 and add line 3, so $3 x_3 = 0$.\n",
    "3. Plug $x_3=0$ into line 3, so $x_2=0$.\n",
    "\n",
    "So, the only solution to $\\textbf{A}{x'}=\\textbf{0}$ for this matrix is $\\textbf{0}$. By the above fundamental theorem, that means $size(\\textbf{A}) = dim(col(\\textbf{A}))$, so our matrix is full-rank!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving through inversion\n",
    "\n",
    "Another approach we could take to solve $\\textbf{A}\\textbf{x}=\\textbf{b}$ is to make use of the matrix inverse $\\textbf{A}^{-1}$. The inverse is defined by the following relationship\n",
    "\n",
    "$$\n",
    "\\textbf{A}^{-1} \\textbf{A} = \\textbf{I}\n",
    "$$\n",
    "\n",
    "With an inverse in hand, we can solve the above linear systems in a simple way\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{A} \\textbf{x} &= \\textbf{b} \\\\\n",
    "\\textbf{A}^{-1} \\textbf{A} \\textbf{x} &= \\textbf{A}^{-1} \\textbf{b} \\\\\n",
    "\\textbf{I} \\textbf{x} &= \\textbf{A}^{-1} \\textbf{b} \\\\\n",
    "\\textbf{x} &= \\textbf{A}^{-1} \\textbf{b} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But which matrices have an inverse? Crucially, they require a trivial null space, where $\\textbf{x}'=\\textbf{0}$ is the only solution to $\\textbf{A}\\textbf{x'}=\\textbf{0}$. Why? A non-trivial null space means there's no inverse that satisfies $\\textbf{x'}=\\textbf{A}^{-1} \\textbf{0}$, since all the elements of $null(\\textbf{A})$ are potential values for $\\textbf{x'}$.\n",
    "\n",
    "*In practice: Computing an inverse to solve a system of equations is usually computationally more expensive and can result in numerical issues, so it can be preferable to work directly with a linear system solver.*\n",
    "\n",
    "*Fun Fact: We were able to use the same sequence of simplification steps to solve our equations as we changed the values of $\\textbf{b}$. It turns out that simplifying a matrix in this way can also be adapted to invert matrices, and is related to certain matrix decompositions. Read more on [Wikipedia](https://en.wikipedia.org/wiki/Gaussian_elimination#Definitions_and_example_of_algorithm).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In practice: Solving the full-rank matrix\n",
    "\n",
    "Here, we'll run code to solve our nicely behaved, full-rank matrix, showing our solutions match the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking our inverse works well. Should close to the identity:\n",
      "[[ 1.00e+00 -2.22e-16  0.00e+00]\n",
      " [ 0.00e+00  1.00e+00 -5.55e-17]\n",
      " [ 0.00e+00  0.00e+00  1.00e+00]]\n",
      "Solution by inverse: [1. 1. 1.]\n",
      "\n",
      "Solution by using a solver: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, -1, 2],\n",
    "    [0, 1, 1],\n",
    "])\n",
    "b = np.array([3, 2, 2])\n",
    "\n",
    "Ainv = np.linalg.inv(A)\n",
    "print('Checking our inverse works well. Should close to the identity:')\n",
    "print(Ainv @ A)\n",
    "print('Solution by inverse:', Ainv @ b)\n",
    "print()\n",
    "\n",
    "print('Solution by using a solver:', np.linalg.solve(A, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Keeping our symbols as above, linear regression is formulated as finding a minimum-error weight vector $\\textbf{x}^*$ of size $m$ that can weight the columns of a feature matrix $\\textbf{A}$ of size $n \\times m$ to reproduce an expected output $\\textbf{b}$ of size $m$. This matrix $\\textbf{A}$ is a design matrix, with $n$ observations as rows and $m$ features as columns. Formally,\n",
    "\n",
    "$$\n",
    "\\textbf{x}^* = \\arg\\min_x \\|\\textbf{A}\\textbf{x} - \\textbf{b}\\|_2\n",
    "$$\n",
    "\n",
    "By taking a derivative to solve for the maximum of this expression (derivation below), we get the following equation, which resembles the simple approach to solving a linear system of equations we used above.\n",
    "\n",
    "$$\n",
    "\\textbf{x}^* = (\\textbf{A}^T\\textbf{A})^{-1}\\textbf{A}^T\\textbf{b}\n",
    "$$\n",
    "\n",
    "This crucially depends on being able to take the inverse of $\\textbf{A}^T\\textbf{A}$, which is of size $m \\times m$. For rectangular matrices, the maximum possible rank is $\\min(m, n)$, so that gives us a few distinct cases to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When $n \\ge m$, or more observations than features\n",
    "\n",
    "Having more observations than features is the typical setting for linear regression.\n",
    "When $n \\ge m$, the matrix rank can be at most $m$, so $rank(\\textbf{A}) \\le m$.\n",
    "If the features are linearly independent, then $rank(\\textbf{A}) = m$, so $rank(\\textbf{A}^T\\textbf{A}) = m$, which makes it invertible, so we can compute the solution $\\textbf{x}^*$ above. When $n > m$, we won't generally be able to find a solution so that $\\textbf{A}\\textbf{x}=\\textbf{b}$, but our solution will have minimum error.\n",
    "\n",
    "Let's think more about the geometry of these solutions. Here's an example matrix\n",
    "\n",
    "$$\n",
    "\\textbf{A} = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "-1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can find a perfect solution for some $\\textbf{b}$, like $\\begin{bmatrix} 2 & -2 & 3 & -3 \\end{bmatrix}$ when $\\textbf{x}=\\begin{bmatrix} 2 & 3 \\end{bmatrix}$. These $\\textbf{b}$ with a perfect solution must be in $col(\\textbf{A})$, and are unique since our columns are linearly independent. However, notice that our $\\textbf{x}$ can have at most two elements -- this means we can only generate a two-dimensional output space, even though the size of $\\textbf{b}$ is four! So, there are many $\\textbf{b}$ that we _can't_ solve exactly, like $\\begin{bmatrix} 7 & -6 & 3 & -4 \\end{bmatrix}$. But, you can get a close solution $\\textbf{x}=\\begin{bmatrix} 6.5 & 3.5 \\end{bmatrix}$. So, regression looks for an approximation of $\\textbf{b}$ that's actually in $col(\\textbf{A})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact solution to b=array([ 2, -2,  3, -3]): x=array([2., 3.]) residuals=[0. 0. 0. 0.]\n",
      "approximate solution to b=array([ 7, -6,  3, -4]): x=array([6.5, 3.5]) residuals=[-0.5 -0.5  0.5  0.5]\n"
     ]
    }
   ],
   "source": [
    "def solve(A, b):\n",
    "    return np.linalg.inv(A.T @ A) @ A.T @ b\n",
    "\n",
    "A = np.array([\n",
    "    [1, 0],\n",
    "    [-1, 0],\n",
    "    [0, 1],\n",
    "    [0, -1],\n",
    "])\n",
    "\n",
    "b = np.array([2, -2, 3, -3])\n",
    "x = solve(A, b)\n",
    "print(f'exact solution to {b=}: {x=} residuals={A@x-b}')\n",
    "\n",
    "b = np.array([7, -6, 3, -4])\n",
    "x = solve(A, b)\n",
    "print(f'approximate solution to {b=}: {x=} residuals={A@x-b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, things are different if our features are linearly dependent. In that case, $rank(\\textbf{A}^T\\textbf{A})<m$, so we can't invert the matrix $\\textbf{A}^T\\textbf{A}$ to compute a solution. Intuitively, it's no longer possible to find a unique solution, since some columns can be rewritten in terms of others. So even though we can solve some problems and approximate the rest, having linearly dependent columns means there are always infinite solutions, whether they are exact or approximate solutions.\n",
    "\n",
    "A related issue can also occur with columns that are collinear, but not entirely linearly dependent. In particular, even when $\\textbf{A}^T\\textbf{A}$ can be inverted, correlated columns can make it difficult to interpret the weights $\\textbf{x}^*$. For example, in a simple case with two nearly-identical columns, any combination of those two columns will result in similar error, which will cause estimates of their standard deviations to be large. While the model on the whole can still achieve low error, it might be difficult to understand which of the two columns is contributing most because of their relationship. Highly collinear columns can also cause numerical issues with inverting $\\textbf{A}^T\\textbf{A}$. More details are on [Wikipedia](https://en.wikipedia.org/wiki/Multicollinearity#Consequences), or in Cosma Shalizi's [course notes (in section 5)](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/14/lecture-14.pdf).\n",
    "\n",
    "*Exercise: We assumed $rank(\\textbf{A}) = rank(\\textbf{A}^T\\textbf{A})$ above. Can you prove this? One approach is to show any element of $null(\\textbf{A})$ is in $null(\\textbf{A}^T \\textbf{A})$, as well as the opposite. [Here's a proof](https://math.stackexchange.com/a/349966) that takes this approach.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When $n < m$, or more features than observations\n",
    "\n",
    "This corresponds to cases where, intuitively, you would be overfitting data when fititng a regression model. We examine these issues by using linear algebra.\n",
    "\n",
    "When $n < m$, the matrix rank can be at most $n$, so $rank(\\textbf{A}) \\le n$. But $size(\\textbf{A}^T\\textbf{A})$ is $m \\times m$. That means that $rank(\\textbf{A}^T\\textbf{A}) \\le n < m$, so $\\textbf{A}^T\\textbf{A}$ is not full rank, so we can't take an inverse. This also makes sense intuitively; having more features than observations means that some features will be linearly dependent, so there are infinite solutions. When $rank(\\textbf{A}) < n$, it is also possible to have no solutions, since there are fewer linearly independent vectors than there are observations. Intuitively, we can think of these cases as analogous to our problematic matrix above.\n",
    "\n",
    "For our example, let's consider a matrix really similar to the one above\n",
    "\n",
    "$$\n",
    "\\textbf{A} = \\begin{bmatrix}\n",
    "1 & -1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This matrix has linearly dependent columns (Can you give an example?), but does have a rank of $n=2$, since we have two linearly independent columns. So it has infinite solutions for any $\\textbf{b}$, like $\\begin{bmatrix} 3 & 4 \\end{bmatrix}$, which is solved by $\\textbf{x}=\\begin{bmatrix} 3 & 0 & 4 & 0 \\end{bmatrix} + c \\begin{bmatrix} 1 & 1 & 1 & 1 \\end{bmatrix}$ for any weight $c$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In practice: Solving the above examples\n",
    "\n",
    "Here, we solve the various examples in this notebook. We sometimes have different results between our simple least squares solution, and `np.linalg.lstsq`. This happens because `np.linalg.lstsq` uses a more robust inverse than our simple approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "\n",
      "example = good\n",
      "\tA=array([[ 1,  1,  1],\n",
      "       [ 1, -1,  2],\n",
      "       [ 0,  1,  1]])\n",
      "\tb=array([3, 2, 2])\n",
      "| Solving with solver_inv...\n",
      "\tx=array([1., 1., 1.])  |  approx b=A@x=array([3., 2., 2.])  |  residuals=[-4.44e-16 -4.44e-16 -2.22e-16]  |  sum squared error=0.00\n",
      "| Solving with solver_solve...\n",
      "\tx=array([1., 1., 1.])  |  approx b=A@x=array([3., 2., 2.])  |  residuals=[0. 0. 0.]  |  sum squared error=0.00\n",
      "| Solving with solver_lstsq_simple...\n",
      "\tx=array([1., 1., 1.])  |  approx b=A@x=array([3., 2., 2.])  |  residuals=[ 1.78e-15  1.33e-15 -4.44e-16]  |  sum squared error=0.00\n",
      "| Solving with solver_lstsq...\n",
      "\trank=3\n",
      "\tx=array([1., 1., 1.])  |  approx b=A@x=array([3., 2., 2.])  |  residuals=[ 4.44e-16 -4.44e-16 -4.44e-16]  |  sum squared error=0.00\n",
      "\n",
      "----------\n",
      "\n",
      "example = bad-infinite\n",
      "\tA=array([[ 1,  1,  1],\n",
      "       [ 1, -1,  2],\n",
      "       [ 2,  0,  3]])\n",
      "\tb=array([3, 2, 5])\n",
      "| Solving with solver_inv...\n",
      "\tCould not solve with solver_inv. Error: Singular matrix\n",
      "| Solving with solver_solve...\n",
      "\tCould not solve with solver_solve. Error: Singular matrix\n",
      "| Solving with solver_lstsq_simple...\n",
      "\tCould not solve with solver_lstsq_simple. Error: Invalid inverse, should be identity: ATAinv @ ATA=array([[3., 0., 6.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 0.]])\n",
      "| Solving with solver_lstsq...\n",
      "\trank=2\n",
      "\tx=array([1., 1., 1.])  |  approx b=A@x=array([3., 2., 5.])  |  residuals=[ 8.88e-16 -2.22e-16  8.88e-16]  |  sum squared error=0.00\n",
      "\n",
      "----------\n",
      "\n",
      "example = bad-contradiction\n",
      "\tA=array([[ 1,  1,  1],\n",
      "       [ 1, -1,  2],\n",
      "       [ 2,  0,  3]])\n",
      "\tb=array([3, 2, 1])\n",
      "| Solving with solver_inv...\n",
      "\tCould not solve with solver_inv. Error: Singular matrix\n",
      "| Solving with solver_solve...\n",
      "\tCould not solve with solver_solve. Error: Singular matrix\n",
      "| Solving with solver_lstsq_simple...\n",
      "\tCould not solve with solver_lstsq_simple. Error: Invalid inverse, should be identity: ATAinv @ ATA=array([[3., 0., 6.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 0.]])\n",
      "| Solving with solver_lstsq...\n",
      "\trank=2\n",
      "\tx=array([0.52, 0.71, 0.43])  |  approx b=A@x=array([1.67, 0.67, 2.33])  |  residuals=[-1.33 -1.33  1.33]  |  sum squared error=5.33\n",
      "\n",
      "----------\n",
      "\n",
      "example = linreg-exact\n",
      "\tA=array([[ 1,  0],\n",
      "       [-1,  0],\n",
      "       [ 0,  1],\n",
      "       [ 0, -1]])\n",
      "\tb=array([ 2, -2,  3, -3])\n",
      "| Solving with solver_inv...\n",
      "\tCould not solve with solver_inv. Error: Last 2 dimensions of the array must be square\n",
      "| Solving with solver_solve...\n",
      "\tCould not solve with solver_solve. Error: Last 2 dimensions of the array must be square\n",
      "| Solving with solver_lstsq_simple...\n",
      "\tx=array([2., 3.])  |  approx b=A@x=array([ 2., -2.,  3., -3.])  |  residuals=[0. 0. 0. 0.]  |  sum squared error=0.00\n",
      "| Solving with solver_lstsq...\n",
      "\trank=2\n",
      "\tx=array([2., 3.])  |  approx b=A@x=array([ 2., -2.,  3., -3.])  |  residuals=[-4.44e-16  4.44e-16 -4.44e-16  4.44e-16]  |  sum squared error=0.00\n",
      "\n",
      "----------\n",
      "\n",
      "example = linreg-approx\n",
      "\tA=array([[ 1,  0],\n",
      "       [-1,  0],\n",
      "       [ 0,  1],\n",
      "       [ 0, -1]])\n",
      "\tb=array([ 7, -6,  3, -4])\n",
      "| Solving with solver_inv...\n",
      "\tCould not solve with solver_inv. Error: Last 2 dimensions of the array must be square\n",
      "| Solving with solver_solve...\n",
      "\tCould not solve with solver_solve. Error: Last 2 dimensions of the array must be square\n",
      "| Solving with solver_lstsq_simple...\n",
      "\tx=array([6.5, 3.5])  |  approx b=A@x=array([ 6.5, -6.5,  3.5, -3.5])  |  residuals=[-0.5 -0.5  0.5  0.5]  |  sum squared error=1.00\n",
      "| Solving with solver_lstsq...\n",
      "\trank=2\n",
      "\tx=array([6.5, 3.5])  |  approx b=A@x=array([ 6.5, -6.5,  3.5, -3.5])  |  residuals=[-0.5 -0.5  0.5  0.5]  |  sum squared error=1.00\n",
      "\n",
      "----------\n",
      "\n",
      "example = linreg-linear-dependence\n",
      "\tA=array([[ 1,  0,  1],\n",
      "       [-1,  0, -1],\n",
      "       [ 0,  1,  1],\n",
      "       [ 0, -1, -1]])\n",
      "\tb=array([ 7, -6,  3, -4])\n",
      "| Solving with solver_inv...\n",
      "\tCould not solve with solver_inv. Error: Last 2 dimensions of the array must be square\n",
      "| Solving with solver_solve...\n",
      "\tCould not solve with solver_solve. Error: Last 2 dimensions of the array must be square\n",
      "| Solving with solver_lstsq_simple...\n",
      "\tCould not solve with solver_lstsq_simple. Error: Singular matrix\n",
      "| Solving with solver_lstsq...\n",
      "\trank=2\n",
      "\tx=array([3.17, 0.17, 3.33])  |  approx b=A@x=array([ 6.5, -6.5,  3.5, -3.5])  |  residuals=[-0.5 -0.5  0.5  0.5]  |  sum squared error=1.00\n",
      "\n",
      "----------\n",
      "\n",
      "example = linreg-overfitting\n",
      "\tA=array([[ 1, -1,  0,  0],\n",
      "       [ 0,  0,  1, -1]])\n",
      "\tb=array([3, 4])\n",
      "| Solving with solver_inv...\n",
      "\tCould not solve with solver_inv. Error: Last 2 dimensions of the array must be square\n",
      "| Solving with solver_solve...\n",
      "\tCould not solve with solver_solve. Error: Last 2 dimensions of the array must be square\n",
      "| Solving with solver_lstsq_simple...\n",
      "\tCould not solve with solver_lstsq_simple. Error: Singular matrix\n",
      "| Solving with solver_lstsq...\n",
      "\trank=2\n",
      "\tx=array([ 1.5, -1.5,  2. , -2. ])  |  approx b=A@x=array([3., 4.])  |  residuals=[-8.88e-16 -8.88e-16]  |  sum squared error=0.00\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    dict(\n",
    "        name='good',\n",
    "        A=np.array([\n",
    "            [1, 1, 1],\n",
    "            [1, -1, 2],\n",
    "            [0, 1, 1],\n",
    "        ]),\n",
    "        b=np.array([3, 2, 2]),\n",
    "    ),\n",
    "    dict(\n",
    "        name='bad-infinite',\n",
    "        A=np.array([\n",
    "            [1, 1, 1],\n",
    "            [1, -1, 2],\n",
    "            [2, 0, 3],\n",
    "        ]),\n",
    "        b=np.array([3, 2, 5]),\n",
    "    ),\n",
    "    dict(\n",
    "        name='bad-contradiction',\n",
    "        A=np.array([\n",
    "            [1, 1, 1],\n",
    "            [1, -1, 2],\n",
    "            [2, 0, 3],\n",
    "        ]),\n",
    "        b=np.array([3, 2, 1]),\n",
    "    ),\n",
    "    dict(\n",
    "        name='linreg-exact',\n",
    "        A=np.array([\n",
    "            [1, 0],\n",
    "            [-1, 0],\n",
    "            [0, 1],\n",
    "            [0, -1],\n",
    "        ]),\n",
    "        b=np.array([2, -2, 3, -3]),\n",
    "    ),\n",
    "    dict(\n",
    "        name='linreg-approx',\n",
    "        A=np.array([\n",
    "            [1, 0],\n",
    "            [-1, 0],\n",
    "            [0, 1],\n",
    "            [0, -1],\n",
    "        ]),\n",
    "        b=np.array([7, -6, 3, -4]),\n",
    "    ),\n",
    "    dict(\n",
    "        name='linreg-linear-dependence',\n",
    "        A=np.array([\n",
    "            [1, 0, 1],\n",
    "            [-1, 0, -1],\n",
    "            [0, 1, 1],\n",
    "            [0, -1, -1],\n",
    "        ]),\n",
    "        b=np.array([7, -6, 3, -4]),\n",
    "    ),\n",
    "    dict(\n",
    "        name='linreg-overfitting',\n",
    "        A=np.array([\n",
    "            [1, -1, 0, 0],\n",
    "            [0, 0, 1, -1],\n",
    "        ]),\n",
    "        b=np.array([3, 4]),\n",
    "    ),\n",
    "]\n",
    "\n",
    "def solver_inv(A, b):\n",
    "    Ainv = np.linalg.inv(A)\n",
    "    assert np.allclose(np.eye(A.shape[0]), Ainv @ A), f'Invalid inverse, should be identity: {Ainv@A=}'\n",
    "    return Ainv @ b\n",
    "\n",
    "def solver_solve(A, b):\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "def solver_lstsq_simple(A, b):\n",
    "    ATA = A.T @ A\n",
    "    ATAinv = np.linalg.inv(ATA)\n",
    "    assert np.allclose(np.eye(ATA.shape[0]), ATAinv @ ATA), f'Invalid inverse, should be identity: {ATAinv @ ATA=}'\n",
    "    return ATAinv @ A.T @ b\n",
    "\n",
    "def solver_lstsq(A, b):\n",
    "    x, resid, rank, singular = np.linalg.lstsq(A, b, rcond=1e-5)\n",
    "    print(f'\\t{rank=}')\n",
    "    return x\n",
    "\n",
    "for example in examples:\n",
    "    A = example['A']\n",
    "    b = example['b']\n",
    "\n",
    "    print()\n",
    "    print('-' * 10)\n",
    "    print()\n",
    "    print('example =', example['name'])\n",
    "    print(f'\\t{A=}')\n",
    "    print(f'\\t{b=}')\n",
    "\n",
    "    for solver in [solver_inv, solver_solve, solver_lstsq_simple, solver_lstsq]:\n",
    "        try:\n",
    "            print(f'| Solving with {solver.__name__}...')\n",
    "            x = solver(A, b)\n",
    "            resid = A@x-b\n",
    "            print(f'\\t{x=}  |  approx b={A@x=}  |  residuals={resid}  |  sum squared error={resid@resid:.2f}')\n",
    "        except Exception as err:\n",
    "            print(f'\\tCould not solve with {solver.__name__}. Error:', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Regression derivation\n",
    "\n",
    "We briefly derive the above expression for regression, using somewhat informal notation for our derivatives. We want to minimize the squared error of the model's predictions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\textbf{x}) &= \\|\\textbf{A}\\textbf{x} - \\textbf{b}\\|^2_2 \\\\\n",
    "&= (\\textbf{A}\\textbf{x} - \\textbf{b})^T(\\textbf{A}\\textbf{x} - \\textbf{b}) \\\\\n",
    "&= \\textbf{x}^T \\textbf{A}^T \\textbf{A}\\textbf{x} - 2 \\textbf{b}^T\\textbf{A}\\textbf{x} + \\textbf{b}^T\\textbf{b}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, we take the derivative of this loss, with respect to our solution vector $\\textbf{x}$. Since we're taking a derivative with respect to a vector, we'll need to use some multivariate, or matrix, calculus. We start with a few identities that we'll need below (pulled from this [cheat sheet](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf)), which resemble the derivatives you'd take in univariate calculus. The vector $\\textbf{c}$ and matrix $\\textbf{C}$ are constants.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d \\textbf{x}} \\textbf{c} &= \\textbf{0} \\\\\n",
    "\\frac{d}{d \\textbf{x}} \\textbf{x}^T \\textbf{c} &= \\textbf{c} \\\\\n",
    "\\frac{d}{d \\textbf{x}} \\textbf{x}^T \\textbf{C} \\textbf{x} &= 2 \\textbf{C} \\textbf{x} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With those identities in place, we can take the derivative.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d \\textbf{x}} \\mathcal{L}(x) \n",
    "&= \\frac{d}{d \\textbf{x}} \\left[ \\textbf{x}^T \\textbf{A}^T \\textbf{A}\\textbf{x} - 2 \\textbf{x}^T\\textbf{A}^T\\textbf{b} + \\textbf{b}^T\\textbf{b} \\right] \\\\\n",
    "&= 2 \\textbf{A}^T \\textbf{A} \\textbf{x} - 2 \\textbf{A}^T\\textbf{b} \\\\\n",
    "% &= 2 \\textbf{x}^T \\textbf{A}^T \\textbf{A} - 2 \\textbf{b}^T\\textbf{A} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, we use this to solve for a minimum error solution by setting the derivative of the loss to zero.\n",
    "$$\n",
    "\\begin{align}\n",
    "0 &= 2 \\textbf{A}^T \\textbf{A} \\textbf{x}^* - 2 \\textbf{A}^T\\textbf{b} \\\\\n",
    "\\textbf{A}^T \\textbf{A} \\textbf{x}^* &= \\textbf{A}^T\\textbf{b} \\\\\n",
    "\\textbf{x}^* &= (\\textbf{A}^T \\textbf{A})^{-1} \\textbf{A}^T\\textbf{b} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Inspiration and examples taken liberally from *Mathematics for Machine Learning* by Deisenroth, Faisal, and Ong.\n",
    "- *Linear Algebra and its Applications* by Lay\n",
    "- *Linear Algebra Done Right* by Axler\n",
    "- *Introduction to Applied Linear Algebra* by Boyd and Vandenberghe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
