{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO CHECK SIZES\n",
    "\n",
    "Introduce Ax=b. A is square\n",
    "Equivalence between matrix-vector multiplication and linear systems of equations\n",
    "Basis, Nullspace of A, describes why there are no solutions for some Ax=b\n",
    "Fundamental theorem - null space generating set is independent of spanning set?\n",
    "Two cases: Invertible (Ax=b has a unique solution), non-invertible (Ax=b has infinitely many or no solutions)\n",
    "Optional (?): Dedicated time on linear (in)dependence\n",
    "\n",
    "Now, Regression → Ax + b = y. Frame this as Ax=y-b with a non-square A\n",
    "More columns than rows → no solutions, in general.\n",
    "\n",
    "Informal: Solve for Normal equation\n",
    "Interpret terms of normal equation in terms of nullspace (X^TX)\n",
    "Edge case: More columns than rows → infinite solutions\n",
    "Optional: Collinearity / rank-deficiency\n",
    "Optional: Note anything about intercept?\n",
    "Optional: Ill-conditioned matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a system\n",
    "In this notebook, we focus on solving a system of equations. This is ultimately related to regression, but we intentionally focus on this simple presentation to stress some of the concepts around when regression does and does not have solutions.\n",
    "\n",
    "Let's start with this simple system of equations and try to solve it.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 3 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 2 \\\\\n",
    "x_2 + x_3 &= 2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*Exercise: Try solving this system of equations. The answer is below.\n",
    "\n",
    "1. We can solve for $x_1$ by subtracting the line 3 from line 1, so $x_1=1$.\n",
    "2. We can plug that into line 2, so our system of equations is now\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 &= 1 \\\\\n",
    "- x_2 + 2 x_3 &= 1 \\\\\n",
    "x_2 + x_3 &= 2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "3. We add line 3 to line 2. That leaves us with $3 x_3 = 3$, so $x_3 = 1$.\n",
    "4. That means $x_2=1$! So our solution $\\mathbf{x} = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$.\n",
    "\n",
    "We just followed an ad-hoc algorithm to solve this matrix, but there are more systematic ways to solve it by 1) systematically repeating simplification steps (called Gaussian eliminiation) until 2) each row reaches a _canonical_ form (reduced row-echelon form). We won't cover this in any detail, but just point out that there are systematic algorithms that can do it, with preferable properties (efficiency, numerical stability).\n",
    "\n",
    "*Fun History Fact from [Wikipedia](https://en.wikipedia.org/wiki/Gaussian_elimination#History):*\n",
    "> *The method of Gaussian elimination appears – albeit without proof – in the Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 AD, but parts of it were written as early as approximately 150 BC.It was commented on by Liu Hui in the 3rd century.*\n",
    "\n",
    "What do these systems of equations have in common with linear regression? They have some clear similarities (solving for linear weights). But also some clear differences (regression is noisy, rarely do we have the same number of observations and features). The goal of these notes is to understand the linear algebra behind regression, in order to understand more about regression and when it does or doesn't have solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more problematic system\n",
    "\n",
    "Now, let's examine a different linear system to see some of the different kinds of solutions they can have.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 3 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 2 \\\\\n",
    "2 x_1 + 3 x_3 &= 1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If you add line 2 to line 1, you get $2 x_1 + 3 x_3 = 5$, which is a contradiction! So, there's no way to solve this system of equations, making them inconsistent. But what happens if we change this to avoid that contradiction? If we only modify the last equation to avoid the contradiction, we get this system (change in bold)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 3 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 2 \\\\\n",
    "2 x_1 + 3 x_3 &= \\textbf{5} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "From this, we can reduce to two equations, obtaining the first by subtracting line 2 from line 1\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "2 x_2 - x_3 &= 1 \\\\\n",
    "2 x_1 + 3 x_3 &= 5 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Any $\\textbf{x}$ that satisfies these equations is a solution, like $\\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$ or $\\begin{bmatrix} -2 & 2 & 3 \\end{bmatrix}$. In fact, there are infinitely many solutions!\n",
    "\n",
    "So, what's special about this system of equations? In our example in the previous section, the system of equations seemed to have a single unique solution. In this example, depending on the $\\textbf{b}$ on the right, its possible to have either zero or infinite solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The many forms of matrix multiplication\n",
    "\n",
    "In this section, we write out some notation to relate the above systems of equations with the matrix equation $\\textbf{A}\\mathbf{x}=\\mathbf{b}$.\n",
    "\n",
    "First, some notation about matrices. Consider a matrix $\\textbf{A}$ of size $n \\times m$, which means it has $n$ rows and $m$ columns.\n",
    "We can write it out in terms of the individual entries of the matrix. The entry at row $i$ and column $j$ is $a_{ij}$, so the matrix is\n",
    "\n",
    "$$\n",
    "\\textbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\ldots & a_{1m} \\\\\n",
    "a_{21} & a_{22} & \\ldots & a_{2m} \\\\\n",
    "\\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "a_{n1} & a_{n2} & \\ldots & a_{nm} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Another way we can write it is in terms of the columns. The $i^{th}$ column is $\\textbf{a}_i$, so the matrix is\n",
    "\n",
    "$$\n",
    "\\textbf{A} = \\begin{pmatrix} \\textbf{a}_1 & \\textbf{a}_2 & \\ldots & \\textbf{a}_m \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So, if we go back to our above example equations, we can rewrite them using some of these notational ideas. Here's the system of equations from the first section:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 3 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 2 \\\\\n",
    "x_2 + x_3 &= 2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Each of these equations can correspond to a dot-product between $\\textbf{x}$ and a vector with the factors (e.g., $\\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$ for the first row). So, we can write this as a matrix equation\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & -1 & 2 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "\\end{bmatrix} \\mathbf{x} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Another different approach is to write it as a vector equation, using the columns of the above matrix.\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} x_1 +\n",
    "\\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} x_2 +\n",
    "\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} x_3\n",
    "= \\begin{bmatrix} 3 \\\\ 2 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "All three of these representations are equivalent, and are all solved by $\\textbf{x} = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$. We will change between them to emphasize different points below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining the contradiction\n",
    "\n",
    "So, to be specific, when we were solving system of equations above, we were equivalently trying to find a solution $\\textbf{x}$ that satisfies the\n",
    "the equation $\\textbf{A}\\textbf{x}=\\textbf{b}$, given a specific matrix $\\textbf{A}$ and vector $\\textbf{b}$. From our above examples, a natural followup is to generalize to arbitrary $\\textbf{b}$ by asking: Given a matrix $\\textbf{A}$, can we say something general about the $\\textbf{b}$ that will have a solution?\n",
    "\n",
    "Let's look at our problematic example above, now in matrix form:\n",
    "$$\n",
    "\\textbf{A} = \\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & -1 & 2 \\\\\n",
    "2 & 0 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can rewrite the equation $\\textbf{A}\\textbf{x}=\\textbf{b}$ in a vectorized form as follows\n",
    "\n",
    "$$\n",
    "\\textbf{a}_1 x_1 + \\textbf{a}_2 x_2 + \\ldots + \\textbf{a}_m x_m = \\textbf{b}\n",
    "$$\n",
    "\n",
    "Or, more specifically for our current example\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix} x_1 +\n",
    "\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix} x_2 +\n",
    "\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} x_3\n",
    "= \\textbf{b}\n",
    "$$\n",
    "\n",
    "This makes it clear that $\\textbf{b}$ is a **linear combination** of the columns of $\\textbf{A}$. A linear combination is a weighted sum of vectors. In this case, the weights are the elements of $\\textbf{x}$, and the vectors that we combine are the columns of $\\textbf{A}$. A related concept is the **span** of a generating set of vectors, which is the set of all possible linear combinations of the vectors. One final concept: The **column space** of a matrix, $col(\\textbf{A})$ is the span of its columns. So, we can rephrase the above as: vectors $\\textbf{b}$ with a solution must be in $col(\\textbf{A})$, the column space of $\\textbf{A}$.\n",
    "\n",
    "So that explains part of what we saw above: we reached a contradiction with $\\textbf{b} = \\begin{bmatrix} 3 & 2 & 1 \\end{bmatrix}$ because it cannot be expressed as a linear combination of the columns of $\\textbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infinite solutions?\n",
    "\n",
    "In order to explain the infinite number of solutions above, we'll have to introduce the **null space**, or $null(\\textbf{A})$, which are the solutions $\\textbf{x'}$ so that $\\textbf{A}\\textbf{x'} = \\textbf{0}$. Every matrix has the trivial solution $\\textbf{x'}=\\textbf{0}$, but only some matrices have other non-trivial solutions. Given known solutions $\\textbf{x}$ and $\\textbf{x'}$, you can construct another solution to $\\textbf{A}\\textbf{x}=\\textbf{b}$ by adding these equations together, since\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{A}\\textbf{x} + \\textbf{A}\\textbf{x'} &= \\textbf{b} + \\textbf{0} \\\\\n",
    "\\textbf{A}(\\textbf{x} + \\textbf{x'}) &= \\textbf{b} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So that means $\\textbf{x}+\\textbf{x'}$ is another solution. Since knowing the null space is a way to generate infinite solutions, let's see if we can solve $\\textbf{A}\\textbf{x} =\\textbf{0}$ for our above matrices. We'll work with our problematic example in equation form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 0 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 0 \\\\\n",
    "2 x_1 + 3 x_3 &= 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we subtract line 2 from line 1 like we did above, we get the following constraints\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "2 x_2 - x_3 &= 0 \\\\\n",
    "2 x_1 + 3 x_3 &= 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "These constraints are satisfied by solutions like $\\begin{bmatrix} -3 & 1 & 2 \\end{bmatrix}$ or $\\begin{bmatrix} -6 & 2 & 4 \\end{bmatrix}$. So, it looks like the null space is the span of a set containing the vector $\\begin{bmatrix} -3 & 1 & 2 \\end{bmatrix}$.\n",
    "\n",
    "And, if we look at the above case of infinite solutions ($\\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$, $\\begin{bmatrix} -2 & 2 & 3 \\end{bmatrix}$), you'll notice that their difference is actually $\\begin{bmatrix} -3 & 1 & 2 \\end{bmatrix}$. This aligns with what we found above, that given a solution $\\textbf{x}$ to $\\textbf{A}\\textbf{x}=\\textbf{b}$ and a non-trivial null space, you can produce an infinite number of solutions $\\textbf{x} + \\textbf{x}'$ for all $x' \\in null(\\textbf{A})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A fundamental theorem\n",
    "\n",
    "For our problematic matrix, we now have an explanation for cases where we reach the contradiction (the element is not in the span column space) and an explanation for cases where their infinite solutions (the null space is non-trivial, and can generate infinite solutions). These two properties of our problematic metrics are fundamentally related, which we'll explain now. But first, we'll introduce some helpful concepts.\n",
    "\n",
    "A set of vectors is **linearly dependent** if one vector can be written as a linear combination of the others.\n",
    "For example, in our problematic matrix, we can write one column as a linear combination of the other two.\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix} =\n",
    "3 \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix} -\n",
    "2 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*Exercise: How are the coefficients of this linear combination related to elements of the null space above? Why?*\n",
    "\n",
    "So, our three columns are redundant! By removing the vector $\\begin{bmatrix} 1 & -1 & 0 \\end{bmatrix}$, we can remove this redundancy. After removing vectors that can be written as a linear combination of others, the remaining ones are **linearly independent**.\n",
    "\n",
    "The redundancy in columns means that, even though we are working with three-dimensional vectors, our vectors are in a two-dimensional space defined by the linearly independent vectors. To keep these concepts separate, we introduce a new concept: the **dimensionality** of a space, $dim(\\cdot)$. So our problemetic matrix $\\textbf{A}$, which has a linearly independent set of two column vectors, has a two-dimensional column space, which we can write out as $dim(col(\\textbf{A}))=2$.\n",
    "\n",
    "We can now write the fundamental theorem of linear algebra\n",
    "\n",
    "$$\n",
    "size(\\textbf{A}) = dim(col(\\textbf{A})) + dim(null(\\textbf{A}))\n",
    "$$\n",
    "\n",
    "where we have assumed a square matrix $\\textbf{A}$ and the size of the matrix is $size(\\cdot)$. The idea here is that there are only two possible cases for the result of $\\textbf{A}\\textbf{x}$, when $\\textbf{x} \\ne \\textbf{0}$. It either results in $\\textbf{0}$, so $\\textbf{x} \\in null(\\textbf{A})$. Or, it results in a non-trivial $\\textbf{b} \\in col(\\textbf{A})$.\n",
    "\n",
    "One final definition. We give a special name to the dimensionality of the column space of a matrix $\\textbf{A}$: the **rank** of the matrix. So, $rank(\\textbf{A}) = dim(col(\\textbf{A}))$. It's an important concept because it tells us something intrinsic about the matrix: how much information about $\\textbf{x}$ is retained (i.e. mapped to the column space) by the result of $\\textbf{A}\\textbf{x}$. When a matrix has a rank that matches the dimensionality, so that $rank(\\textbf{A})=size(\\textbf{A})$, we call it **full-rank**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The unproblematic matrix\n",
    "\n",
    "So, going back to our very first example, what was special about that system of equations? Let's try to compute its null space.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1 + x_2 + x_3 &= 0 \\\\\n",
    "x_1 - x_2 + 2 x_3 &= 0 \\\\\n",
    "x_2 + x_3 &= 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We follow the same sequence of simplification steps as above, so\n",
    "\n",
    "1. Subtract line 3 from line 1, so $x_1=0$.\n",
    "2. Plug $x_1=0$ into line 2 and add line 3, so $3 x_3 = 0$.\n",
    "3. Plug $x_3=0$ into line 3, so $x_2=0$.\n",
    "\n",
    "So, the only solution to $\\textbf{A}{x'}=\\textbf{0}$ for this matrix is $\\textbf{0}$. By the above fundamental theorem, that means $size(\\textbf{A}) = dim(col(\\textbf{A}))$, so our matrix is full-rank!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving through inversion\n",
    "\n",
    "Another approach we could take to solve $\\textbf{A}\\textbf{x}=\\textbf{b}$ is to make use of the matrix inverse $\\textbf{A}^{-1}$. The inverse is defined by the following relationship\n",
    "\n",
    "$$\n",
    "\\textbf{A}^{-1} \\textbf{A} = \\textbf{I}\n",
    "$$\n",
    "\n",
    "With an inverse in hand, we can solve the above linear systems in a simple way\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{A} \\textbf{x} &= \\textbf{b} \\\\\n",
    "\\textbf{A}^{-1} \\textbf{A} \\textbf{x} &= \\textbf{A}^{-1} \\textbf{b} \\\\\n",
    "\\textbf{I} \\textbf{x} &= \\textbf{A}^{-1} \\textbf{b} \\\\\n",
    "\\textbf{x} &= \\textbf{A}^{-1} \\textbf{b} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "What matrices have an inverse? Crucially, they require a trivial null space, where $\\textbf{x}'=\\textbf{0}$ is the only solution to $\\textbf{A}\\textbf{x'}=\\textbf{0}$. Why? A non-trivial null space means there's no inverse that satisfies $\\textbf{x'}=\\textbf{A}^{-1} \\textbf{0}$, since all the elements of $null(\\textbf{A})$ are potential values for $\\textbf{x'}$.\n",
    "\n",
    "*In practice: Computing an inverse to solve a system of equations is usually computationally more expensive and can result in numerical issues, so it can be preferable to work directly with a linear system solver.*\n",
    "\n",
    "*Fun Fact: We were able to use the same sequence of simplification steps to solve our equations as we changed the values of $\\textbf{b}$. It turns out that simplifying a matrix in this way can also be adapted to invert matrices, and is related to certain matrix decompositions. Read more on [Wikipedia](https://en.wikipedia.org/wiki/Gaussian_elimination#Definitions_and_example_of_algorithm).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "TKTK from VMLS, regression is when we have non-zero error\n",
    "\n",
    "Keeping our symbols as above, linear regression is formulated as finding a minimum-error weight vector $\\textbf{x}^*$ of size $m$ that can weight the columns of a feature matrix $\\textbf{A}$ of size $n \\times m$ to reproduce an expected output $\\textbf{b}$ of size $m$. This matrix $\\textbf{A}$ is a design matrix, with $n$ observations as rows and $m$ features as columns. Formally,\n",
    "\n",
    "$$\n",
    "\\textbf{x}^* = \\argmax_x \\|\\textbf{A}\\textbf{x} - \\textbf{b}\\|_2\n",
    "$$\n",
    "\n",
    "By taking a derivative to solve for the maximum of this expression, we get the following equation, which resembles the simple approach to solving a linear system of equations we used above.\n",
    "\n",
    "$$\n",
    "\\textbf{x}^* = (\\textbf{A}^T\\textbf{A})^{-1}\\textbf{A}^T\\textbf{b}\n",
    "$$\n",
    "\n",
    "This crucially depends on being able to take the inverse of $\\textbf{A}^T\\textbf{A}$, which is of size $m \\times m$. For rectangular matrices, the maximum possible rank is $\\min(m, n)$, so that gives us a few distinct cases to consider.\n",
    "\n",
    "## When $n < m$, or more features than observations\n",
    "\n",
    "When $n < m$, the matrix rank can be at most $n$, so $rank(\\textbf{A}) <= n$. That means that $rank(\\textbf{A}^T\\textbf{A}) <= n < m$, so we can't take an inverse. This also makes sense intuitively; having more features than observations means that some features will be linearly dependent, so there are infinite solutions. When $rank(\\textbf{A}) < n$, it is also possible to have no solutions.\n",
    "\n",
    "*Exercise: We assumed $rank(\\textbf{A}) = rank(\\textbf{A}^T\\textbf{A})$ above. Can you prove this? One approach is to show any element of $null(\\textbf{A})$ is in $null(\\textbf{A}^T \\textbf{A})$, as well as the opposite. [Here's a proof](https://math.stackexchange.com/a/349966) that takes this approach.*\n",
    "\n",
    "## When $n = m$\n",
    "\n",
    "We discussed this pretty extensively above, but TKTK can we say something about overfitting of some kind? Basically, given noise in observation, we wouldn't expect a perfect fit to be right\n",
    "\n",
    "## When $n > m$, or more observations than features\n",
    "\n",
    "When $n > m$, the matrix rank can be at most $m$, so $rank(\\textbf{A}) <= m$.\n",
    "Having more observations than features is the typical setting for linear regression. If the features are linearly independent, then $rank(\\textbf{A}) = m$, so $rank(\\textbf{A}^T\\textbf{A}) = m$, which makes it invertible, so we can compute the solution $\\textbf{x}^*$ above.\n",
    "\n",
    "However, if the features are linearly dependent, we can't invert the matrix $\\textbf{A}^T\\textbf{A}$ to compute a solution. Because of numerical issues (more details on [wikipedia](https://en.wikipedia.org/wiki/Multicollinearity#Consequences)), it is also possible to encounter a related issue with columns that are collinear, but not entirely linearly dependent. In particular, even when $\\textbf{A}^T\\textbf{A}$ can be inverted, it is difficult to interpret the weights $\\textbf{x}^*$ for collinear columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression derivation\n",
    "\n",
    "We briefly derive the above expression for regression, using somewhat informal notation for our derivatives. We start by writing out the loss that we aim to minimize\n",
    "\n",
    "NOTE\n",
    "- square in loss\n",
    "- \"we assume least squares, noting that this is equivalent to the maximum likelihood extimator\"\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\textbf{x}) &= \\|\\textbf{A}\\textbf{x} - \\textbf{b}\\|^2_2 \\\\\n",
    "&= (\\textbf{A}\\textbf{x} - \\textbf{b})^T(\\textbf{A}\\textbf{x} - \\textbf{b}) \\\\\n",
    "&= \\textbf{x}^T \\textbf{A}^T \\textbf{A}\\textbf{x} - 2 \\textbf{b}^T\\textbf{A}\\textbf{x} + \\textbf{b}^T\\textbf{b}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, we take the derivative of this loss, with respect to our solution vector $\\textbf{x}$. Since we're taking a derivative with respect to a vector, we'll need to use some multivariate, or matrix, calculus. We start with a few identities that we'll need below (pulled from this [cheat sheet](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf)), which resemble the derivatives you'd take in univariate calculus. The vector $\\textbf{c}$ and matrix $\\textbf{C}$ are constants.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d \\textbf{x}} \\textbf{c} &= \\textbf{0} \\\\\n",
    "\\frac{d}{d \\textbf{x}} \\textbf{x}^T \\textbf{c} &= \\textbf{c} \\\\\n",
    "\\frac{d}{d \\textbf{x}} \\textbf{x}^T \\textbf{C} \\textbf{x} &= 2 \\textbf{C} \\textbf{x} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With those identities in place, we can take the derivative.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d \\textbf{x}} \\mathcal{L}(x) \n",
    "&= \\frac{d}{d \\textbf{x}} \\left[ \\textbf{x}^T \\textbf{A}^T \\textbf{A}\\textbf{x} - 2 \\textbf{x}^T\\textbf{A}^T\\textbf{b} + \\textbf{b}^T\\textbf{b} \\right] \\\\\n",
    "&= 2 \\textbf{A}^T \\textbf{A} \\textbf{x} - 2 \\textbf{A}^T\\textbf{b} \\\\\n",
    "% &= 2 \\textbf{x}^T \\textbf{A}^T \\textbf{A} - 2 \\textbf{b}^T\\textbf{A} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, we use this to solve for a minimum error solution by setting the derivative of the loss to zero. We won't prove it here, but it can be shown that this loss function has no local minima.\n",
    "$$\n",
    "\\begin{align}\n",
    "0 &= 2 \\textbf{A}^T \\textbf{A} \\textbf{x}^* - 2 \\textbf{A}^T\\textbf{b} \\\\\n",
    "\\textbf{A}^T \\textbf{A} \\textbf{x}^* &= \\textbf{A}^T\\textbf{b} \\\\\n",
    "\\textbf{x}^* &= (\\textbf{A}^T \\textbf{A})^{-1} \\textbf{A}^T\\textbf{b} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Inspiration and examples taken liberally from *Mathematics for Machine Learning* by Deisenroth, Faisal, and Ong.\n",
    "- Also referenced *Linear Algebra and its Applications* by Lay and *Linear Algebra Done Right* by Axler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
