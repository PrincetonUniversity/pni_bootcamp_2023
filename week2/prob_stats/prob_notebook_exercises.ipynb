{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8702b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to have all imports you'll need\n",
    "from scipy.stats import bernoulli, beta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68945d24",
   "metadata": {},
   "source": [
    "# Sampling from distribuions\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "Use the `bernoulli` distribution from `scipy.stats` to sample a coin flip, with probability heads equal to 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78504472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f280c",
   "metadata": {},
   "source": [
    "Write a function that takes in a parameter $p$ denoting the probability of heads, and $n$ the number of coin flips. This function will then generate those $n$ flips and return the result. Test this function by generating $n=1000$ samples with probability $p=0.3$, then plot that result as a histogram. Remember to label your axis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a2d93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f148559",
   "metadata": {},
   "source": [
    "Repeat the above 100 different times, but with $p=0.5$. Then add up all the samples and plot the sum as a histogram again. \n",
    "\n",
    "What distribution does this sum of bernoulli samples histogram look like? Explain why *(hint: look up sum of bernoulli trials, and seperately the centeral limit theorem)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2479a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0372e",
   "metadata": {},
   "source": [
    "# Distributions and link functions - practice coding math\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "Use the equation:\n",
    "\n",
    "\n",
    " $$p(y; x, w, g(\\cdot)) = \\frac{g(x, w)^y e ^{-g(x, w)}}{y!}$$\n",
    " \n",
    " To code up the proability mass function of a poisson distribution. \n",
    " \n",
    " Make sure it's output is the same the pmf of the poisson object from `scipy.stats`, that is\n",
    " \n",
    " `np.isclose(poisson_pmf(y, x, w), poisson.pmf(y, mu=np.exp(x*w)), atol=0.001)` should evaluate to `True`\n",
    " \n",
    " The `math` library contains the factorial function you'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d9c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import what you need first\n",
    "\n",
    "def poisson_pmf(y, x, w):\n",
    "    pass\n",
    "\n",
    "#below should evaluatte to True, if implemented correctly\n",
    "#np.isclose(poisson_pmf(10, 3, 3), poisson.pmf(10, mu=np.exp(3*3)), atol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756cfac2",
   "metadata": {},
   "source": [
    "Repeat the previous exercise but now for a bernoulli distribution\n",
    "\n",
    "$$p(y; x, w, \\sigma(\\cdot)) = \\sigma(xw)^y \\big(1 - \\sigma(xw)\\big)^{1 - y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d42b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_pmf(y, x, w):\n",
    "    pass\n",
    "\n",
    "#below should evaluatte to True, if implemented correctly\n",
    "#np.isclose(bernoulli_pmf(1, 8, 0.6), bernoulli.pmf(1, expit(8*0.6)), atol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce5c4c",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "Recall the mouse in the experiment box scenario from the lecture notebook. The mouse has two levers to choose from, one which delivers food, and another which delivers water. \n",
    "\n",
    "We want to know what the mouses preference for water is on a particular day of the week. \n",
    "\n",
    "* What distribution can be used to model the mouses choice in this experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28996305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the appropriate distribution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ebe62f",
   "metadata": {},
   "source": [
    "Denote the moues preference for water with the parameter $\\theta$, and say that $P(choice = water) = \\theta$ corresponds to the probability that the mouse selects water. Additionally, there is data available. We have observed the mouses choices. We'll code $water = 1$, and $food = 0$\n",
    "\n",
    "$d = \\big(0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ... \\big)$\n",
    "\n",
    "We're going to use the data to help us make an inference on what the preference for water is\n",
    "\n",
    "If we write out our query of wanting to know the mouse's prefrence for water it's: \n",
    "\n",
    "$$P(\\theta | d)$$\n",
    "\n",
    " * now apply Bayes rule (write it in markdown in the cell below):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d8ef8",
   "metadata": {},
   "source": [
    "$$P(\\theta | d) = your answer here$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ab039",
   "metadata": {},
   "source": [
    "* Remember that the denominator $P(d)$ can be written in another way, write that out in the cell below in markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89f19f",
   "metadata": {},
   "source": [
    "$$\n",
    "P(d) = your answer here\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bf8b2",
   "metadata": {},
   "source": [
    "Let's say we've observed the mouse's choices on previous days, and have counted the number of times it choose water and the number of times it choose food. With that information we assign a prior distribution to $P(\\theta)$. Out of 868 choices it chose water 238 times, and food 630 times. We'll use this information to create a prior distribution for $\\theta$ below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1e7aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_water_choices = 358\n",
    "prior_total_choices = 868\n",
    "prior_food_choices = prior_total_choices - prior_water_choices\n",
    "prior_water = beta(prior_water_choices, prior_food_choices)\n",
    "prior_water; # this is a set beta distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce69d9",
   "metadata": {},
   "source": [
    "Let's assume a discrete grid for all the values that $\\theta$ can take on, and set some other important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92bb2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_thetas = np.linspace(0.001, 0.99, 2000)\n",
    "\n",
    "true_water_pref = 0.56 # ground truth\n",
    "\n",
    "data = bernoulli(p=true_water_pref).rvs(600) # generate some data from the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d74a7",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "When you have an array of data points as we do, how do we compute $P(d | \\theta)$? The first thing to realize is that $P(d | \\theta)$ is a shortened version of: \n",
    "\n",
    "$$P(0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ... | \\theta)$$\n",
    "\n",
    "additionally we make an assumption of independence and identitically distributed. This means that we are assuming each sample uses the same distributtion for it's likelihood function, and that each sample is independent of one another (obviously this latter assumption is wrong). \n",
    "\n",
    "Now recall that if two events are independent then: $P(a, b) = P(a)P(b)$. Use this fact to both write out in markdown the from of the likelihood, and code it up. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee4a25",
   "metadata": {},
   "source": [
    "$$P(d | \\theta) = your answer here$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84ba1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's an example P( 1 | theta = 0.5): bernoulli(p=0.5).pmf(1)\n",
    "# use that to computue P(d | theta), for the moment assume a theta of 0.5\n",
    "\n",
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e77441",
   "metadata": {},
   "source": [
    "Repeat the code portion of what you've done above, but this time compute that likelihood for all values in the array `possible_thetas`, then normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59fd05a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e17eaae",
   "metadata": {},
   "source": [
    "Do what you just did again, however this time take into account the prior distribution over $\\theta$. Use the function that was created for you `prior_water`, it has a probability density method associated with it, here's how you'd use it: \n",
    "\n",
    "`prior_water.pdf(theta_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cb323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "842e54fa",
   "metadata": {},
   "source": [
    "Plot the two distributions, and observe the effect of the prior. Use the information you gather from plotting these distributions to say something about the mouses preference for water, explain what the prior does, is it a good prior in this situation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c45d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd023f",
   "metadata": {},
   "source": [
    "Explain yourself here: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c9573",
   "metadata": {},
   "source": [
    "# Modelling - conjugacy\n",
    "\n",
    "## Exercise 4\n",
    "\n",
    "You may have noticed that in the exercise above we aren't exploiting certain algebraic manipulations that give us closed form solutions for our inference problem. Instead, we made an approximation to the integral using a discrete set of points (the 2000 evenly spaced `theta` values). In this exercise you'll use conjugate distributions. \n",
    "\n",
    "In probability theory certain distributions are conjugate to one another. All this means is that they have very similiar looking expressions that allow us to allegbraically manipulate parameters of the distributions. For a concerete example, recall the Bernoulli distribution:\n",
    "\n",
    "$$p(d_i; \\theta) = \\theta^{d_i} (1 - \\theta)^{1 - d_i}$$\n",
    "\n",
    "and here is a Beta distribution, which is conjugate to the Bernoulli family:\n",
    "\n",
    "$$p(\\theta; \\alpha, \\beta) = \\frac{1}{Z} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}$$\n",
    "\n",
    "notice how similar they are to one another. This allows us to algebraically combine similar terms between distributions: \n",
    "\n",
    "$$\\theta^{d_i}\\theta^{\\alpha - 1} = \\theta^{d_i + \\alpha - 1}$$\n",
    "\n",
    "$$(1 - \\theta)^{1 - d_i} (1 - \\theta)^{\\beta - 1} = (1 - \\theta)^{\\beta - d_i}$$\n",
    "\n",
    "which means if we write out $p(d_i; \\theta) \\times p(\\theta; \\alpha, \\beta)$, and for the moment ignore $Z^{-1}$, then we have\n",
    "\n",
    "$$p(d_i; \\theta) p(\\theta; \\alpha, \\beta) \\propto \\theta^{d_i + \\alpha - 1} (1 - \\theta)^{\\beta - d_i}$$\n",
    "\n",
    "Why is this useful? It might look like silly math we are doing, but it turns out that it is exact inference from Bayes rule. \n",
    "\n",
    "Recall: $P(\\theta | d_i) \\propto P(d_i|\\theta)P(\\theta)$, and that is exactly what we have in the statement above. Taking this a step further, let's introduce some variables so that we can explictly write this as posterior distribution (which will be a beta distribution) over $\\theta$, that looks like the Beta distribution initially introduced\n",
    "\n",
    "set: \n",
    "\n",
    "$$\\alpha' = d_i + \\alpha$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\beta' = 1 - d_i + \\beta$$\n",
    "\n",
    "doing so gives us a distribution that is exactly like the initial beta distribution but with updated $\\alpha$ and $\\beta$ parameters: \n",
    "\n",
    "$$P(\\theta | d_i, \\alpha', \\beta') = \\frac{1}{Z} \\theta^{\\alpha' - 1} (1 - \\theta)^{\\beta' - 1}$$\n",
    "\n",
    "one last thing (not super important, but incase you were curious): \n",
    "\n",
    "$$\\frac{1}{Z} = \\frac{\\Gamma(\\alpha' + \\beta')}{\\Gamma(\\alpha')\\Gamma(\\beta')}$$\n",
    "\n",
    "* Your task is to code up and solve this inference problem using the array of data provided in the exercise above. Make use of the math that is shown above to iteratively update the appropriate parameters. Then plot the posterior distribution over $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434dd408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
